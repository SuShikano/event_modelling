---
title: 'Event History Modelling: Cabinet Data'
author: "Susumu Shikano"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


In this markdown, we estimate different event history models by using Alt and Kingâ€™s (1994) data on cabinet failure.

## Preparation


```{r , warning=FALSE}
library(foreign)
library(flexsurv)

setwd("../data")


#rawdata <- read.dta("Event_History_Course_Cabinet.dta")
rawdata <- read.csv("coalition2.csv")

rawdata$country[is.na(rawdata$country)] <- "unknown"
```

We can check the raw data:

```{r}
head(rawdata)

```

The rows correspond to the individual cabinets (n=314). The data contains the following variables:

- duration: duration
- ciep12: dummy variable whether government is terminated by 12 months prior to the constitutional interelection period
- invest: dummy for legal requirement of legislative investiture vote
- fract: fractionalization
- polar: polarization
- numst2: dummy for government majority in the parliament
- crisis: days needed to form the government
- country: country
#- format: formation attempts
#- postelec: dummy for post-election 
#- caretakr: dummy for care taker government

The main outcome variable is $duration$ which measures the duration of the cabinet in month.

```{r}

par(mfrow=c(1,3))
hist(rawdata$duration,br=seq(0,60,5),
     main="All obs.",xlab="Month")
hist(rawdata$duration[rawdata$ciep12==1],br=seq(0,60,5),
     main="Terminated obs.",xlab="Month")
hist(rawdata$duration[rawdata$ciep12==0],br=seq(0,60,5),
     main="Survived til 12M before CIEP",xlab="Month")


```

The left-hand panel shows duration of all cabinets in the dataset. The mid panel shows in contrast only the cabinets which were terminated during a legislative period, more exactly speaking by 12 months before the constitutional interelection period (CIEP). And the right-hand panel shows those which survived until 12 months before CIEP. 


This specific type of the dependent variable can be processed for event modelling by using $Surv$ function by considering the event variable ($ciep12$):

```{r}

Surv(rawdata$duration,rawdata$ciep12)

```

## OLS 

Before estimating the event history models, we can regress the duration variable on the covariates:

```{r}
summary(lm.out <- lm(duration~ invest + polar + 
          numst2 + crisis,
          data=rawdata))
```

This analysis does not consider that some cabinets did not experience failure. Therefore, we can exclude such cases and regress again the same outcome variable on the covariates:

```{r}
only.censored <- rawdata[rawdata$ciep12==1,]
nrow(only.censored)

summary(lm.out.censored <- lm(duration~ invest + polar + 
          numst2 + crisis,
          data=only.censored))
```

If one compares both results, the latter result without cabinets survived until the next election has  smaller effects for some covariates (invest, numst2). While the second analysis shows a better diagnostics result (see below), the analysis does not take into account a significant number of cabinets (n=43) in the dataset.


```{r}
par(mfrow=c(1,2))
plot(lm.out,which=1)
plot(lm.out.censored,which=1)

```


## Estimating parametric models

We first estimate the following six parametric models:

- Exponential
- Weibull
- log normal
- log logistic
- Gompertz
- Generalized gamma.

As covariate, we include the same set of variables (investiture, polarization, government's seat share,
formation, post-election and care taker.) The models differ however in additional parameters, therefore we will have different sets of parameters.

```{r }
model.label <- c("exponential",
              "weibull",
              "lnorm",
              "llogis",
              "gompertz",
              "gengamma")

all.para.out <- vector("list",length(model.label))

for (i.model in 1:length(model.label)){
  
est.res <- flexsurvreg(Surv(duration,ciep12) ~ 
                           invest + polar + 
                          numst2 + crisis,
                         data=rawdata,
                         dist=model.label[i.model])
all.para.out[[i.model]] <- list(model=model.label[i.model],
                                est=est.res)

print("-------------------------------------------")
print(paste("MODEL:",model.label[i.model]))
print(est.res)
}

```





We can plot the hazard functions with 95% confidence intervals (red) against kernel-based smooth nonparametric hazard (black):

```{r}
par(mfrow=c(2,3))
for (i.model in 1:length(model.label)){
  plot(all.para.out[[i.model]]$est,type="hazard",
       main=model.label[i.model])
}
```

We further can plot the survival functions:

```{r}
par(mfrow=c(2,3))
for (i.model in 1:length(model.label)){
  plot(all.para.out[[i.model]]$est,type="survival",
       main=model.label[i.model])
}

```



## Cox PH model

We can now switch to Cox PH model keeping the covariate constant:


```{r}
est.res <- coxph(Surv(duration,ciep12) ~ 
                           invest + polar + 
                          numst2 + crisis,data=rawdata)
print(est.res)


```

We can now check the situation at individual time points:

```{r}
summary(survfit(est.res))

```
At each time point, the table above gives the number of observations at risk, number of the observed events, the resulting survival rate and their variance/interval estimate.

Due to the rough data (unit = month), we have tie events at most time points.


We can now plot the survival function and the cumulative hazard function. The first plot is based on the empirical data, the other plot on the right-hand side is based on the hypothetical situation where:

- there is legal requirement of investiture vote.
# - there is no polarization.
- the cabinet has a parliamentary majority.
- the coalition negotiation took 30 days.
#- the cabinet has formed after the first attempt right after a parliamentary election.
#- the cabinet is not a care taker government.

```{r}

par(mfrow=c(2,2))
plot(survfit(est.res),main="Empirical data: Kaplan-Meier",ylab="Survival")
plot(survfit(est.res,
             newdata=data.frame(invest=1,polar=0,numst2=1,crisis=30
                                )),main="Empirical data",ylab="Survival")

plot(survfit(est.res),fun="cumhaz",main="Counter factual",ylab="Cumulative hazard")
plot(survfit(est.res,
             newdata=data.frame(invest=1,polar=0,numst2=1,crisis=30
                                )),fun="cumhaz",main="Counter factual",ylab="Cumulative hazard")


```


Both upper panels show the survival function based on the empirical data (the left panel) and the counter-factual scenario stated above (the right panel). The left-hand side plot is based on the simple Kaplan-Meier-estimator: $\hat S (t) = \prod_{i=1}^{t-1} \left(1-\frac{d_i}{n_i} \right)$ with $i \in \{1, \ldots, K\}$ being the time points when at least one event was observed. $d_i$ is the number of event at $t_i$ and $n_i$ is the size of the risk set at $t_i$.  

The lower panels show the cumultative hazard function. Due to the proportional hazard property, both functions have the same form, but they differ in the scale (see the y-axis of both panels.)

We can also compare the effect of requirement of investiture votes where the other variables are kept to be the mean:

```{r}

par(mfrow=c(1,2))
new.dat <- data.frame(invest=c(0,1), polar=rep(mean(rawdata$polar),2), 
                       fract=rep(mean(rawdata$fract),2),
                       numst2=rep(mean(rawdata$numst2),2),
                       crisis=rep(mean(rawdata$crisis),2))
plot(survfit(est.res,newdata=new.dat), 
                            lty=c(1,2),col=c("red","blue"))
legend("topright",col=c("red","blue"),lty=c(1,2),c("No investiture","Investiture required"),bty="n")
plot(survfit(est.res,newdata=new.dat),conf.int=T, 
                            lty=c(1,2),col=c("red","blue"))


```


As we have seen above, there are many tie events due to the duration measure based on month. To deal with such tie events, there are several possibilities, among which the following solutions are implemented in the survival package:

- Efron method (default and the result above)
- Breslow method
- Exact partial likelihood


```{r}


est.res <- coxph(Surv(duration,ciep12) ~ invest + polar + 
              numst2 + crisis,data=rawdata,
              ties="efron")
print(est.res)


est.res <- coxph(Surv(duration,ciep12) ~ invest + polar + 
              numst2 + crisis,data=rawdata,
              ties="breslow")
print(est.res)

est.res <- coxph(Surv(duration,ciep12) ~ invest + polar + 
              numst2 + crisis,data=rawdata,
              ties="exact")
print(est.res)

```

In our case, fortunately, the different methods do not produce any significant differences between the estimates.



## Discrete time modelling

### Converting data into the counting process style

We convert the original data into the counting process format. For this purpose, the survSplit function is useful:


```{r}
id <- 1:dim(rawdata)[1]
rawdata.with.id <- cbind(id,rawdata)

# --------------------------------------------------------------------------
# For time varying covariates
cut.points <- seq(0.5,max(rawdata$duration),by=0.5)
cp.data <- survSplit(data = rawdata.with.id, cut = cut.points, 
     end = "duration",start = "time0", event = "ciep12")

cp.data <- cp.data[order(cp.data$id),]

head(cp.data,n=10)

```

For comparison, you can check the original data again:

```{r}
head(rawdata.with.id)
```


### Binary logit model with counting process data

To the counting process data, we can just apply the binary logit model to estimate an event model:

```{r}

est.res <- glm(ciep12 ~ invest + polar + 
              numst2 + crisis ,data=cp.data,family=binomial(link="logit"))
summary(est.res)

```



The above model is however based on the assumption that the hazard is constant over time, which is quite restrictive. This can be relaxed by using time depending variables.

The most simple approach is to add the dummy variable for all individual time points:

```{r}
est.res <- glm(ciep12 ~ invest + polar + numst2 + crisis +
              factor(duration)
              ,data=cp.data,family=binomial(link="logit"))
summary(est.res)

intercept <- coef(est.res)["(Intercept)"]
time.dummy <- coef(est.res)[substr(names(coef(est.res)),1,16)=="factor(duration)"]
time.hazard <- exp(intercept + time.dummy)

plot(seq(1,59,by=0.5),time.hazard,type="l")

```
This model requires a large amount of dummy variables. The estimated time dependent hazard is based on a few data points. 

Alternaitvely, we can include the duration variable itself.


```{r}
est.res <- glm(ciep12 ~ invest + polar + numst2 + crisis  +
              duration
              ,data=cp.data,family=binomial(link="logit"))
summary(est.res)

intercept <- coef(est.res)["(Intercept)"]
time.coef <- coef(est.res)["duration"]
time.hazard <- exp(intercept + time.coef * (0:max(cp.data$duration)))

plot(c(0:max(rawdata$duration)),time.hazard,type="l")

```

Note that the estimated time dependent hazard is not linear due to the binary logit model.

The duration variable can be also added after logarithmization:

```{r}
# ---------------------------------------------------------------------------------------------------
# natural log
est.res <- glm(ciep12 ~ invest + polar + numst2 + crisis +
              log(duration)
              ,data=cp.data,family=binomial(link="logit"))
summary(est.res)

intercept <- coef(est.res)["(Intercept)"]
time.coef <- coef(est.res)["log(duration)"]
time.hazard <- exp(intercept + time.coef * log((0:max(cp.data$duration))))

plot(c(0:max(cp.data$duration)),time.hazard,type="l")

```

Further, the duration variable can be considered in the quadratic function:

```{r}
durat2 <- cp.data$duration^2
cp.data <- cbind(cp.data,durat2)
est.res <- glm(ciep12 ~ invest + polar + numst2 + crisis +
              duration + durat2
              ,data=cp.data,family=binomial(link="logit"))
summary(est.res)

intercept <- coef(est.res)["(Intercept)"]
time.coef <- coef(est.res)["duration"]
time.coef2 <- coef(est.res)["durat2"]
time.hazard <- exp(intercept + time.coef * (0:max(cp.data$duration)) + time.coef2 * (0:max(cp.data$duration))^2 )

plot(c(0:max(cp.data$duration)),time.hazard,type="l")

```

### Cox PH model with count process data

Now we can estimate Cox PH model by using conditional logit model. For this purpose, we first convert the rawdata in the format for clogit. This data is different from the above data:

```{r}
# ---------------------------------------------------------------------------------------------------
# For Cox regression
# ---------------------------------------------------------------------------------------------------
cut.points <- unique(rawdata$durat[rawdata$censor == 1])
cox.cp.data <- survSplit(data = rawdata.with.id, cut = cut.points, 
     end = "duration",start = "time0", event = "ciep12")


head(cp.data,n=20)
head(cox.cp.data,n=20)


```

Now we can estimate the Cox model by using clogit:

```{r}
clogit(ciep12 ~ invest + polar + 
              numst2 + crisis +
              strata(time0) ,
              method="exact",
              data=cox.cp.data)

```

For comparison, we can estimate again the same model by using coxph().


```{r}
# ---------------------------------------------------------------------------------------------------
# Comparison with coxph()
est.res <- coxph(Surv(duration,ciep12) ~ invest + polar + 
              numst2 + crisis,
              ties="exact", 
              data=rawdata)
print(est.res)

```



## Flexible parametric model: Royceton-Palmer

We first estimate the spline with k=0, which is equivalent to Weibul model.


```{r}


est.res.k0 <- flexsurvspline(Surv(duration,ciep12) ~ invest + polar + 
                   numst2 + crisis ,data=rawdata,
                   k=0,scale="hazard")
print(est.res.k0)



```

For comparison, here is the result of the above Weibull model:

```{r}
flexsurvreg(formula =Surv(duration,ciep12) ~ invest + polar + 
                   numst2 + crisis, data = rawdata, dist = "weibullph")
```

Note that here the parametrization is based on PH and not AFT like the estimate above.

Now, we can introduce the knot point, which varies between 1 and 10:

```{r}

all.royceton <- vector("list",11)
all.royceton[[1]] <- est.res.k0

for (k in 1:10){
  all.royceton[[k+1]] <- flexsurvspline(Surv(duration,ciep12) ~ invest + polar + 
                   numst2 +  crisis,data=rawdata,
                   k=k,scale="hazard")

}

all.para.label <- names(all.royceton[[11]]$coefficients)

all.results <- NULL
for (k in 0:10){
  temp.result <- cbind(all.royceton[[k+1]]$coefficients,sqrt(diag(all.royceton[[k+1]]$cov))) 
  key <- match(all.para.label , rownames(temp.result))
  all.results <- cbind(all.results,temp.result[key,])
}

rownames(all.results) <- all.para.label

col.label  <- paste0("k=",0:10)

colnames(all.results) <- rep("",22)
colnames(all.results)[seq(1,21,by=2)] <- col.label

print(all.results,digits=3)

```




```{r}
plot(all.royceton[[1]],ci=TRUE,type="hazard",col="red")

lines(all.royceton[[2]],ci=TRUE,type="hazard",col="blue")

lines(all.royceton[[7]],ci=TRUE,type="hazard"
      ,col="green")
legend("topleft",lty=1,col=c("red","blue","green"),c("k=0","k=1","k=6"),bty="n")
```

## Model diagnostics


Again, we first estimate a Cox model:

```{r}
est.res <- coxph(Surv(duration,ciep12) ~ invest + polar + 
              numst2 + crisis,
              ties="exact", 
              data=rawdata)
print(est.res)

```

Most diagnostics rely on the residuals from the result above. Event history models know various kinds of residuals. For  the output object produced by **coxph()** we can use **resid()** to easily obtain the following residuals:

- martingale 
- deviance
- score
- schoenfeld
- dfbeta
- dfbetas
- scaledsch
- partial



### Cox Snell residuals

Cox-Snell residuals are defined as the estimated integrated hazard: $r_{CS_i}= \hat H_i = \exp(\hat{\beta}'{x}_i) \hat{H}_0(t_i)$ with $\hat{H}_0(t_i) = - \log \hat{S}_0(t) = - \sum_i \log \left( \sum_{j \in R(t_i)} e^{\beta' {x}_j}\right)$ (integrated baseline hazard).  

There is no option for Cox-Snell in **resid()**, however it can be obtained from Martingale residuals due to the following relationship between them: $ M_i  = \delta_i(t) - r_{CS_i} $.  


```{r}
### calculate cox snell from martingale
cox.snell <- rawdata$ciep12-resid(est.res,type="martingale")

plot(density(cox.snell[rawdata$ciep12==1],from=0,to=5),ylim=c(0,1),ylab="Density",xlab="Cox Snell Residuals",main="")
exponential.func <- function(x) dexp(x,1)
curve(exponential.func,0,5,ylab="f(x)",add=T,col="red",lty=2)

```

If the model is correct, Cox-Snell residuals should be distributed according to the unit exponential distribution (red dotted line in the figure). If it is the case, the cumulative hazard based on the Cox-Snell residuals $H_r(r_{CS})$ should correspond to the Cox-Snell residuals themselves. To obtain $H_r(r_{CS})$, we can use the relationship: $H_r(r_{CS}) = - \log S_r(r_{CS})$. The corresponding survival function based on Cox-Snell can be obtained by using the Kaplan-Meier-estimator.


```{r}
## estimate the cumulative hazard function for residuals;
H.t <- survfit(coxph(Surv(cox.snell,rawdata$ciep12)~1,method='breslow'),stype=2,ctype=2)

par(mfrow=c(1,2))
plot(H.t$time,-log(H.t$surv),type='l',xlab='Cox-Snell Residuals', 
	ylab="H(t) = -log S(t)",
	main="")
abline(0,1,col='red',lty=2)

hist(cox.snell,main="Distribution of Cox-Snell Residuals",xlab="Cox-Snell Residuals")

```

The red dotted line in the left-hand panel is the 45-degree line. If the plot is close to the line, Cox-Snell residuals are likely to be distributed according to the unit exponential. There is a larger deviation from the 45-degree line in the range with $r_{CS}>4$, however as the right-hand panel shows, there are only one case there. 


#### Martingale Residuals 

Martingale residuals are more intuitive than the Cox-Snell residuals. If the data is represented the counting process, the Martingale residuals are defined as: $M_i(t) = \delta_i - H_i(t) = \delta_i - r_{CS_i}$. 


```{r, warning=FALSE}

res.m <- residuals(est.res,type="martingale")
par(mfrow=c(2,2))
plot(res.m ~ rawdata$invest,ylab="Martingale Residuas",xlab="Investiture")
abline(lm(res.m ~ rawdata$invest))
abline(h=0,col="red")
plot(res.m ~ rawdata$numst2,ylab="Martingale Residuas",xlab="Majority")
abline(lm(res.m ~ rawdata$numst2))
abline(h=0,col="red")

scatter.smooth(rawdata$polar,res.m,ylab="Martingale Residuas",xlab="Polarization")
abline(h=0,col="red")
scatter.smooth(rawdata$crisis,res.m,ylab="Martingale Residuas",xlab="Length of negotiation")
abline(h=0,col="red")

```

The expected martingale residuals conditioned by the different covariates are close to zero, which implies no need to change the linear function in  the model.


#### Poorly prediced obaservations based on deviance residuals

Martingale residuals have an asymmetrical range between $-\infty$ and $1$, therefore they are strongly skewed. Deviance residuals are rescaled Martingale, which is more symmetrical: $D_i= sign(M_i(t))\sqrt{-2 \left[ M_i(t) + \delta_i \log(\delta_i-M_i(t) ) \right]}$.


```{r}
res.dev <- residuals(est.res,type="deviance")

par(mfcol=c(2,3))
scatter.smooth(res.m,ylab="Martingale",xlab="obs",main="Martingale")
abline(h=0,col="red")
hist(res.m,main="",xlab="Martingale")
scatter.smooth(res.dev,ylab="Deviance",xlab="obs",main="Deviance")
abline(h=0,col="red")
hist(res.dev,main="",xlab="Deviance")

plot(res.m,res.dev ,xlab="Martingale",ylab="Deviance")



```


To detect the poorly predicted observations, we can plot the deviance residuals against the duration time:

```{r}
scatter.smooth(rawdata$duration,res.dev,xlab="Duration",ylab="Deviance residuals")
abline(h=0,lty=2)

#par(mfrow=c(1,2))
#scatter.smooth(rawdata$polar,res.dev)
#scatter.smooth(rawdata$crisis,res.dev)

```

The plot gives a systematic relationship between residuals and duration. The cabinets with a short duration tend to have positive deviance residuals, which means these cabinets are predicted to survive longer than observed. Conversely, the cabinets which survived longer tend to be predicted to survive shorter. This can indicate a possible violation of PH property since there is a systematic difference in residuals over time.


#### Outlier detection based on the score residuals

The score residuals are obtained case- and covariate-specific. Given we are interested in the covariate $X$, $i$'s case residual is in the counting process representation: $L_i = \int^\infty_0 [X_i(t) - \bar{X}_i (t)] d\hat{M}_i(t)$. $\bar{X}_i (t)$ is the weighted mean of $X$ in the risk set at $t$, and $d\hat{M}_i(t)$ is the change in martingale residuals for $i$ at $t$.

```{r}
est.res <- coxph(Surv(duration,ciep12) ~ invest + polar + 
              numst2 + crisis,
              #ties="exact", ## score is not available for the exact method for tie events.
              data=rawdata)

res.score <- residuals(est.res,type="score")

par(mfrow=c(2,2))
for (i in 1:4){
  plot(res.score[,i],type="l",xlab="obs.",ylab="Score residuals",main=names(coef(est.res))[i])
  abline(h=0,col="white")
}

```

Here we see some observations which have a stronger impact on the coefficients, but it is hard to interpret the value itself. It is easier to interpret if we standardize the scale:

```{r}

res.dfbeta <- residuals(est.res,type="dfbeta")

par(mfrow=c(2,2))
for (i in 1:4){
  plot(res.dfbeta[,i],type="l",xlab="obs.",ylab="Standardized score residuals",main=names(coef(est.res))[i])
  abline(h=0,col="white")
}

```

The scales on the y-axes inform that the impact of individual cases is quite limited and its highest impact changes only 2% of the coefficients. 



#### Testing the PH property via piecewise regression

The most straightforward way to check the PH assumption is to repeat the analysis based on the subsamples with different duration.


Here, we can divide the data into two subdata with the almost same size:

- duration shorter than 15 months (n=`r sum(rawdata$duration<15)`)
- duration not shorter than 15 months (n=`r sum(rawdata$duration>=15)`)

```{r}
all.res <- NULL
for (i in 1:3){
  
    if (i==1) this.data <- rawdata
    if (i==2) this.data <- rawdata[rawdata$duration<15,]
    if (i==3) this.data <- rawdata[rawdata$duration>=15,]
    est.res <- coxph(Surv(duration,ciep12) ~ invest + polar + 
                  numst2 + crisis,
                  ties="exact", 
                  data=this.data)
    print(c("All observations","duration < 15","duration >= 15")[i])
    print(est.res)
    print("-------------------------------------------")
    all.res <- rbind(all.res,coef(summary(est.res))[,c(1,3)])
}

```

The estimates can be compared in the following figure:

```{r}

source("https://raw.githubusercontent.com/SuShikano/SusumuMiscR/main/R/Coef_CI_Plot.R")

all.res <- all.res[c(1,5,9,2,6,10,3,7,11,4,8,12),]
all.res[c(4:6,10:12),] <- all.res[c(4:6,10:12),]*10
rownames(all.res)[c(2,3,5,6,8,9,11,12)] <- rep(c("<15",">=15"),4)


coef.ci(estimates = all.res[,1],se=all.res[,2],
        var.labels = rownames(all.res),xlab="Estimates")
abline(v=0,lty=2)
```

Accordingly, two variables, polarization and majority status, have smaller, and no significant effects on the risk if we only consider the duration under 15 months. This indicates possible violation of the PH assumption.


#### Testing the PH property via Schoenfeld Residuals

While the above piecewise regression is straightforward, its result depends on how one divide the data. Alternatively, we can also utilize Schoenfeld residuals to check the PH assumption.

```{r,fig.dim = c(8, 8)}

est.res <- coxph(Surv(duration,ciep12) ~ invest + polar + 
              numst2 + crisis,
              ties="exact", 
              data=rawdata)

par(mfrow=c(2,2))
plot(cox.zph(est.res, transform="identity"))


```
According to the plot, there are no clear patterns of residuals over time, which indicates no violation of the PH assumption. The results can also be quantified in a statistical test:


```{r}

test.res <- cox.zph(est.res, transform="identity")
test.res
```
Accordingly, none of the covariates nor the global test indicates any significant violation of the PH assumption.


## Heterogeneity in the data

### Robust standard error based on the Sandwitch estimator


```{r}
table(rawdata$country)
```

The individual cabinets included in the dataset here are clustered in countries, while we have not considered any country-specific factors. This might have caused some errors in the estimates and underestimation of their standard errors in the  results of the naive Cox model below. 

```{r}

est.res <- coxph(Surv(duration,ciep12) ~ invest + polar + 
              numst2 + crisis,
              robust=FALSE,
              data=rawdata)
print(est.res)

```

To deal with possible heterogeneity, we can estimate the robust standard errors by using the Sandwich-estimator:


```{r}

est.res <- coxph(Surv(duration,ciep12) ~ invest + polar + 
              numst2 + crisis,
              robust=TRUE,
              data=rawdata)
print(est.res)

```

Further, we can also take into account the clustering based on countries:


```{r}

est.res <- coxph(Surv(duration,ciep12) ~ invest + polar + 
              numst2 + crisis,
              cluster=country,
              data=rawdata)
print(est.res)

```



### Frailty model


In the above analysis, we only corrected the standard errors. We can now estimate another model by introducing frailty.

```{r}
library(frailtyEM)

est.res.frail <- emfrail(Surv(duration,ciep12) ~ invest + polar + 
              numst2 + crisis + cluster(country),
                data=rawdata, distribution=emfrail_dist(dist="gamma"))
summary(est.res.frail)

```




```{r}
par(mfcol=c(1,2))
plot(est.res.frail,
     newdata=data.frame(invest=1,polar=0,numst2=1,crisis=30
                                ),
     type="pred",type_pred = "marginal",
     main="Investiture required",ylim=c(0,2),xlim=c(0,50))
par(new=T)
plot(survfit(est.res,
             newdata=data.frame(invest=1,polar=0,numst2=1,crisis=30
                                )),fun="cumhaz",main="Naive Cox",ylab="Cumulative hazard",ylim=c(0,2),xlim=c(0,50),col="red",ann=F,axes=F)

plot(est.res.frail,
     newdata=data.frame(invest=0,polar=0,numst2=1,crisis=30
                                ),
     type="pred",,type_pred = "marginal",
     main="No investiture",ylim=c(0,2))
par(new=T)

plot(survfit(est.res,
             newdata=data.frame(invest=0,polar=0,numst2=1,crisis=30
                                )),fun="cumhaz",ylab="Cumulative hazard",ylim=c(0,2),xlim=c(0,50),col="red",ann=F,axes=F)

legend("topleft",col=c("black","red"),lty=1,c("Frailty","Naive"),bty="n")
```

